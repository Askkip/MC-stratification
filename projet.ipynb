{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import qmc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9510565162951535)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(u):\n",
    "    \"\"\"\n",
    "    u est un vecteur de taille d>=1 -> np.array\n",
    "    \"\"\"\n",
    "    d = len(u)\n",
    "    somme = np.sum(u)\n",
    "    return np.cos(2*np.pi*(1/d * somme -0.5))\n",
    "\n",
    "f(np.array([1,0.5,0.3,0.4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\int_{[0,1]^d}^{}{f(u)du}  \\simeq \\frac{1}{N}\\sum_{i=1}^{N}f(u_{(i)}) \\text{ with } u_{(i)} \\sim^{iid} U([0,1]^d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Célian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\stats\\_qmc.py:958: UserWarning: The balance properties of Sobol' points require n to be a power of 2.\n",
      "  sample = self._random(n, workers=workers)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension d = 1\n",
      "  N = 100: Monte Carlo = 0.015085, Quasi-Monte Carlo = -0.015158\n",
      "  N = 1000: Monte Carlo = 0.019122, Quasi-Monte Carlo = 0.001125\n",
      "  N = 10000: Monte Carlo = 0.005975, Quasi-Monte Carlo = 0.000141\n",
      "  N = 20000: Monte Carlo = 0.001760, Quasi-Monte Carlo = 0.000002\n",
      "\n",
      "Dimension d = 2\n",
      "  N = 100: Monte Carlo = 0.471487, Quasi-Monte Carlo = 0.414825\n",
      "  N = 1000: Monte Carlo = 0.416602, Quasi-Monte Carlo = 0.405535\n",
      "  N = 10000: Monte Carlo = 0.406130, Quasi-Monte Carlo = 0.405363\n",
      "  N = 20000: Monte Carlo = 0.406469, Quasi-Monte Carlo = 0.405307\n",
      "\n",
      "Dimension d = 3\n",
      "  N = 100: Monte Carlo = 0.536279, Quasi-Monte Carlo = 0.580652\n",
      "  N = 1000: Monte Carlo = 0.551938, Quasi-Monte Carlo = 0.567298\n",
      "  N = 10000: Monte Carlo = 0.568819, Quasi-Monte Carlo = 0.565590\n",
      "  N = 20000: Monte Carlo = 0.563573, Quasi-Monte Carlo = 0.565538\n",
      "\n",
      "Dimension d = 5\n",
      "  N = 100: Monte Carlo = 0.759878, Quasi-Monte Carlo = 0.712243\n",
      "  N = 1000: Monte Carlo = 0.698254, Quasi-Monte Carlo = 0.714866\n",
      "  N = 10000: Monte Carlo = 0.717299, Quasi-Monte Carlo = 0.716303\n",
      "  N = 20000: Monte Carlo = 0.716750, Quasi-Monte Carlo = 0.716359\n",
      "\n",
      "Dimension d = 10\n",
      "  N = 100: Monte Carlo = 0.862908, Quasi-Monte Carlo = 0.836673\n",
      "  N = 1000: Monte Carlo = 0.863487, Quasi-Monte Carlo = 0.849533\n",
      "  N = 10000: Monte Carlo = 0.847342, Quasi-Monte Carlo = 0.847904\n",
      "  N = 20000: Monte Carlo = 0.849375, Quasi-Monte Carlo = 0.847924\n",
      "\n",
      "Dimension d = 100\n",
      "  N = 100: Monte Carlo = 0.982984, Quasi-Monte Carlo = 0.983029\n",
      "  N = 1000: Monte Carlo = 0.983025, Quasi-Monte Carlo = 0.983571\n",
      "  N = 10000: Monte Carlo = 0.984031, Quasi-Monte Carlo = 0.983657\n",
      "  N = 20000: Monte Carlo = 0.983702, Quasi-Monte Carlo = 0.983693\n",
      "\n",
      "Dimension d = 200\n",
      "  N = 100: Monte Carlo = 0.993072, Quasi-Monte Carlo = 0.990269\n",
      "  N = 1000: Monte Carlo = 0.991952, Quasi-Monte Carlo = 0.991887\n",
      "  N = 10000: Monte Carlo = 0.991689, Quasi-Monte Carlo = 0.991852\n",
      "  N = 20000: Monte Carlo = 0.991892, Quasi-Monte Carlo = 0.991799\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Monte Carlo standard\n",
    "def monte_carlo_integration(d, N):\n",
    "    samples = np.random.uniform(0, 1, (N, d))\n",
    "    return np.mean(np.array([f(sample) for sample in samples]))\n",
    "\n",
    "# Quasi-Monte Carlo (Sobol sequence)\n",
    "def quasi_monte_carlo_integration(d, N):\n",
    "    sobol = qmc.Sobol(d, scramble=True)\n",
    "    samples = sobol.random(N)\n",
    "    return np.mean(np.array([f(sample) for sample in samples]))\n",
    "\n",
    "# Différentes valeurs de d et N\n",
    "d_values = [1, 2, 3,5, 10, 100, 200]\n",
    "N_values = [100, 1000, 10000, 20000]\n",
    "\n",
    "# Stocker les résultats\n",
    "results = {}\n",
    "\n",
    "for d in d_values:\n",
    "    results[d] = {}\n",
    "    for N in N_values:\n",
    "        mc_result = monte_carlo_integration(d, N)\n",
    "        qmc_result = quasi_monte_carlo_integration(d, N)\n",
    "        results[d][N] = (mc_result, qmc_result)\n",
    "\n",
    "# Affichage des résultats\n",
    "for d in results:\n",
    "    print(f\"Dimension d = {d}\")\n",
    "    for N in results[d]:\n",
    "        mc, qmc_res = results[d][N]\n",
    "        print(f\"  N = {N}: Monte Carlo = {mc:.6f}, Quasi-Monte Carlo = {qmc_res:.6f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quand $d \\to \\infty$, on remarque que $I \\to 1$ \\\n",
    "Pour la dimension $d=2$ par exemple, on peut calculer l'intégrale double et on trouve $I = \\frac{4}{\\pi^2} \\simeq 0.40528$ et ça nous permet de remarquer que QMC converge plus vite vers le bon résultat.\\\n",
    "De même pour $d=1$, $I = 0$ par changement de variable, et on voit la vitesse de convergence plus rapide de QMC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "e_{0,k} = \\left\\{ \\left( \\frac{2j_1 + 1}{2k}, \\dots, \\frac{2j_s + 1}{2k} \\right) \n",
    "\\text{ s.t. } (j_1, \\dots, j_s) \\in \\{0, \\dots, k-1\\}^s \\right\\}\n",
    "\\ $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$\\\n",
    "\\hat{I}_{1,k}(f) := \\frac{1}{k^s} \\sum_{c \\in \\mathcal{e}_0,k} f(c + U_c), \\quad U_c \\sim \\mathcal{U} \\left( \\left[ -\\frac{1}{2k}, \\frac{1}{2k} \\right]^s \\right)\n",
    "\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def haber_estimator_1(f, k, s):\n",
    "    \"\"\"\n",
    "    Compute Haber's first estimator (order 1).\n",
    "    \n",
    "    Parameters:\n",
    "        f: function to integrate\n",
    "        k: number of subintervals per dimension\n",
    "        s: dimension of the domain\n",
    "    \n",
    "    Returns:\n",
    "        Approximate integral value using Haber's first estimator.\n",
    "    \"\"\"\n",
    "    centers = np.array(np.meshgrid(*[np.linspace(1/(2*k), 1 - 1/(2*k), k) for _ in range(s)])).T.reshape(-1, s)\n",
    "    U = np.random.uniform(-1/(2*k), 1/(2*k), size=centers.shape)\n",
    "    return np.mean(f(centers + U))\n",
    "\n",
    "\n",
    "def haber_estimator_2(f, k, s):\n",
    "    \"\"\"\n",
    "    Compute Haber's second estimator (order 2).\n",
    "    \n",
    "    Parameters:\n",
    "        f: function to integrate\n",
    "        k: number of subintervals per dimension\n",
    "        s: dimension of the domain\n",
    "    \n",
    "    Returns:\n",
    "        Approximate integral value using Haber's second estimator.\n",
    "    \"\"\"\n",
    "    centers = np.array(np.meshgrid(*[np.linspace(1/(2*k), 1 - 1/(2*k), k) for _ in range(s)])).T.reshape(-1, s)\n",
    "    U = np.random.uniform(-1/(2*k), 1/(2*k), size=centers.shape)\n",
    "    return np.mean((f(centers + U) + f(centers - U)) / 2)\n",
    "\n",
    "\n",
    "# Test function\n",
    "def test_function(x):\n",
    "    return np.cos(2 * np.pi * (np.mean(x, axis=1) - 0.5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension d = 1\n",
      "  N = 10: Haber1 = 0.062587, Haber2 = -0.000516\n",
      "  N = 20: Haber1 = 0.008122, Haber2 = 0.000835\n",
      "  N = 50: Haber1 = -0.001822, Haber2 = -0.000022\n",
      "  N = 100: Haber1 = 0.000455, Haber2 = 0.000000\n",
      "\n",
      "Dimension d = 2\n",
      "  N = 100: Haber1 = 0.396897, Haber2 = 0.405147\n",
      "  N = 400: Haber1 = 0.403108, Haber2 = 0.405288\n",
      "  N = 2500: Haber1 = 0.405498, Haber2 = 0.405281\n",
      "  N = 10000: Haber1 = 0.405291, Haber2 = 0.405283\n",
      "\n",
      "Dimension d = 3\n",
      "  N = 1000: Haber1 = 0.567892, Haber2 = 0.565688\n",
      "  N = 8000: Haber1 = 0.566278, Haber2 = 0.565572\n",
      "  N = 125000: Haber1 = 0.565560, Haber2 = 0.565597\n",
      "  N = 1000000: Haber1 = 0.565592, Haber2 = 0.565596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_values = [1, 2, 3]\n",
    "k_values = [10, 20, 50, 100]  # Number of subintervals per dimension\n",
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "for d in d_values:\n",
    "    results[d] = {}\n",
    "    for k in k_values:\n",
    "        I1 = haber_estimator_1(test_function, k, d)\n",
    "        I2 = haber_estimator_2(test_function, k, d)\n",
    "        results[d][k] = (I1,I2)\n",
    "\n",
    "# Affichage des résultats\n",
    "for d in results:\n",
    "    print(f\"Dimension d = {d}\")\n",
    "    for k in results[d]:\n",
    "        mc, qmc_res = results[d][k]\n",
    "        print(f\"  N = {k**d}: Haber1 = {mc:.6f}, Haber2 = {qmc_res:.6f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence d'Haber1 et d'Haber2 vers la vraie valeur de l'intégrale beaucoup plus rapide, par exemple en dimension 1 dès N=20 on est est déjà proche de 0 alors qu'il fallait attendre N=100 dans la Q1. \\\n",
    "\n",
    "On observe aussi une convergence plus rapide de la variance d'Haber2 grâce à la technique de réduction de la variance.\n",
    "\n",
    "Plus k augmente, plus N augmente et plus l'estimateur d'Haber converge vers la valeur de l'intégrale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension d = 5\n",
      "  N = 32: Haber1 = 0.720534, Haber2 = 0.726921\n",
      "  N = 243: Haber1 = 0.721927, Haber2 = 0.716624\n",
      "  N = 1024: Haber1 = 0.713877, Haber2 = 0.717298\n",
      "\n",
      "Dimension d = 10\n",
      "  N = 1024: Haber1 = 0.845533, Haber2 = 0.847042\n",
      "  N = 59049: Haber1 = 0.848460, Haber2 = 0.847936\n",
      "  N = 1048576: Haber1 = 0.847851, Haber2 = 0.847855\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_values = [5,10]\n",
    "k_values = [2,3,4]  # Number of subintervals per dimension\n",
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "for d in d_values:\n",
    "    results[d] = {}\n",
    "    for k in k_values:\n",
    "        I1 = haber_estimator_1(test_function, k, d)\n",
    "        I2 = haber_estimator_2(test_function, k, d)\n",
    "        results[d][k] = (I1,I2)\n",
    "\n",
    "# Affichage des résultats\n",
    "for d in results:\n",
    "    print(f\"Dimension d = {d}\")\n",
    "    for k in results[d]:\n",
    "        mc, qmc_res = results[d][k]\n",
    "        print(f\"  N = {k**d}: Haber1 = {mc:.6f}, Haber2 = {qmc_res:.6f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Understanding the Approximate Distribution \n",
    "\n",
    "We analyze the distribution of the quantity:\n",
    "\n",
    "$$\n",
    "S_d = \\frac{1}{d} \\sum_{i=1}^{d} u_i\n",
    "$$\n",
    "\n",
    "when $( d )$ is large. Given that $( u_i \\sim U(0,1) )$ (i.i.d.), we apply the **Central Limit Theorem (CLT)**. Since the expectation and variance of a uniform $( U(0,1) )$ variable are:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[u_i] = \\frac{1}{2}, \\quad \\text{Var}[u_i] = \\frac{1}{12},\n",
    "$$\n",
    "\n",
    "we obtain:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[S_d] = \\frac{1}{d} \\sum \\mathbb{E}[u_i] = \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Var}[S_d] = \\frac{1}{d^2} \\sum \\text{Var}[u_i] = \\frac{1}{12d}\n",
    "$$\n",
    "Then we have,\n",
    "\n",
    "$$ \\sqrt{d} \\times \\frac{S_{d} - \\frac{1}{2}}{1/12} \\simeq \\mathcal{N} \\left( 0, 1 \\right)$$\n",
    "\n",
    "Thus, for large $( d )$, we approximate:\n",
    "\n",
    "$$\n",
    "S_d \\sim \\mathcal{N} \\left( \\frac{1}{2}, \\frac{1}{12d} \\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step  2 : Importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance Sampling\n",
    "$$\\mathbb{E}_{x \\sim P}[f(x)] = \\mathbb{E}_{x \\sim Q}\\Big[f(x)\\frac{P(x)}{Q(x)}\\Big]$$\n",
    "Which means  $\\mathbb{E}_{x \\sim P}[f(x)] \\approx \\frac{1}{n}\\sum_{i=1}^nf(x_i)\\frac{P(x_i)}{Q(x_i)}$ where $x_i$ are drawn from $Q$. This applies when $P$ and $Q$ are both normalized. For unnormalized case (not our case)   \n",
    "$$\\mathbb{E}_{x \\sim P}[f(x)] \\approx \\frac{\\sum_{i=1}^nf(x_i)\\frac{P(x_i)}{Q(x_i)}}{\\sum_{i=1}^n\\frac{P(x_i)}{Q(x_i)}}$$  \n",
    "Let the proposal distribution $Q(x)$ be a normal distribution.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal, norm\n",
    "\n",
    "def uniform_pdf(u,d):\n",
    "    \"\"\"\n",
    "    Densité de probabilité pour une distribution uniforme sur [0,1]^d.\n",
    "    Comme c'est uniforme, la densité est constante et vaut 1 sur l'intervalle.\n",
    "    \"\"\"\n",
    "    if np.all((u >= 0) & (u <= 1)):  # Vérifie que u est bien dans [0,1]^d\n",
    "        return 1\n",
    "    else:\n",
    "        return 0  # En dehors de [0,1]^d, la densité est nulle\n",
    "\n",
    "def normal_pdf(u,d):\n",
    "    \"\"\"\n",
    "    Densité de probabilité pour une loi normale multidimensionnelle de dimension d  : N(mu, Sigma)\n",
    "    avec mu = (1/2, ..., 1/2) et Sigma = (1/12d) * I_d (matrice identité).\n",
    "    \"\"\"\n",
    "    mu = np.full(d, 0.5)  # Vecteur moyenne [1/2, 1/2, ..., 1/2]\n",
    "    sigma2 = 1 / (12 * d)  # Variance pour chaque variable\n",
    "    sigma = np.eye(d) * sigma2  # Matrice de covariance diagonale\n",
    "\n",
    "    return multivariate_normal.pdf(u, mean=mu, cov=sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IS_MC(N,d, mu=1/2, sigma = None):\n",
    "    if not sigma :\n",
    "        sigma = 1/(12*d)\n",
    "    somme = 0\n",
    "    for i in range(N):\n",
    "        sample = np.random.multivariate_normal(np.full(d, 0.5), np.eye(d) *1 / (12 * d))\n",
    "        somme += f(sample) * (uniform_pdf(sample,d)/normal_pdf(sample,d))\n",
    "    return somme/N\n",
    "\n",
    "def IS_QMC(N,d, mu=1/2, sigma = None):\n",
    "    if not sigma :\n",
    "        sigma = 1/(12*d)\n",
    "    \"\"\" Importance Sampling avec Quasi-Monte Carlo utilisant une séquence Sobol \"\"\"\n",
    "    sampler = qmc.Sobol(d, scramble=True)  # Génère des points Sobol dans [0,1]^d\n",
    "    U = sampler.random(N)  # Matrice (N, d) avec des points uniformes\n",
    "\n",
    "    # Transformation en loi normale N(0.5, 1/(12d)) avec la fonction quantile\n",
    "    mu = 0.5\n",
    "    sigma = np.sqrt(1 / (12 * d))\n",
    "    X = norm.ppf(U, loc=mu, scale=sigma)  # Transforme U ~ U[0,1] en X ~ N(mu, sigma^2)\n",
    "\n",
    "\n",
    "    # Importance Sampling\n",
    "    somme = 0\n",
    "    for x in X:\n",
    "        somme += f(x) * (uniform_pdf(x,d) /normal_pdf(x, d))  # p(u) = 1 pour uniforme\n",
    "    return somme / len(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension d = 1\n",
      "  N = 1000: IS MC = -0.064902, IS QMC = -0.003318\n",
      "  N = 5000: IS MC = -0.002290, IS QMC = -0.000282\n",
      "  N = 10000: IS MC = 0.021875, IS QMC = 0.000097\n",
      "  N = 20000: IS MC = 0.010941, IS QMC = -0.000030\n",
      "  N = 50000: IS MC = -0.007276, IS QMC = -0.000001\n",
      "\n",
      "Dimension d = 2\n",
      "  N = 1000: IS MC = 0.382461, IS QMC = 0.384455\n",
      "  N = 5000: IS MC = 0.419227, IS QMC = 0.406609\n",
      "  N = 10000: IS MC = 0.415513, IS QMC = 0.396052\n",
      "  N = 20000: IS MC = 0.401890, IS QMC = 0.401476\n",
      "  N = 50000: IS MC = 0.410526, IS QMC = 0.405488\n",
      "\n",
      "Dimension d = 3\n",
      "  N = 1000: IS MC = 0.512919, IS QMC = 0.684261\n",
      "  N = 5000: IS MC = 0.539900, IS QMC = 0.569808\n",
      "  N = 10000: IS MC = 0.630148, IS QMC = 0.444802\n",
      "  N = 20000: IS MC = 0.579459, IS QMC = 0.597402\n",
      "  N = 50000: IS MC = 0.561716, IS QMC = 0.560892\n",
      "\n",
      "Dimension d = 5\n",
      "  N = 1000: IS MC = 2.673726, IS QMC = 0.416223\n",
      "  N = 5000: IS MC = 0.460763, IS QMC = 1.318448\n",
      "  N = 10000: IS MC = 0.466999, IS QMC = 0.512339\n",
      "  N = 20000: IS MC = 0.411859, IS QMC = 20.686740\n",
      "  N = 50000: IS MC = 0.621461, IS QMC = 0.651488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_values = [1, 2, 3, 5]\n",
    "N_values = [1000, 5000, 10000, 20000, 50000]  # Number of subintervals per dimension\n",
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "for d in d_values:\n",
    "    results[d] = {}\n",
    "    for N in N_values:\n",
    "        I1 = IS_MC( N, d)\n",
    "        I2 = IS_QMC( N, d)\n",
    "        results[d][N] = (I1,I2)\n",
    "\n",
    "# Affichage des résultats\n",
    "for d in results:\n",
    "    print(f\"Dimension d = {d}\")\n",
    "    for k in results[d]:\n",
    "        mc, qmc_res = results[d][k]\n",
    "        print(f\"  N = {k}: IS MC = {mc:.6f}, IS QMC = {qmc_res:.6f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problèmes et solutions pour l'Importance Sampling\n",
    "\n",
    "## 1. Mauvais choix de la densité d'importance\n",
    "L'Importance Sampling (IS) est très sensible au choix de la densité d'importance $g(x)$.  \n",
    "Si $g(x)$ ne couvre pas bien les régions où $f(x)$ est significatif, les pondérations définies par :\n",
    "\n",
    "$$ w(x) = \\frac{p(x)}{g(x)} $$\n",
    "\n",
    "peuvent devenir extrêmement variables, entraînant une grande variance de l'estimateur.  \n",
    "En grande dimension, une loi normale centrée en $0.5$ peut mal représenter les régions importantes de $f(x)$, ce qui engendre une instabilité des résultats.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Problème de dégénérescence en grande dimension\n",
    "Lorsque la dimension $d$ augmente, la plupart des échantillons issus de $g(x)$ se retrouvent éloignés des régions où $f(x)$ est significatif.  \n",
    "Cela conduit à une situation où très peu d'échantillons contribuent réellement à l'intégrale, ce qui augmente la variance et rend l'estimateur instable.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Inefficacité des méthodes Quasi-Monte Carlo en grande dimension\n",
    "Les méthodes Quasi-Monte Carlo (QMC) offrent un gain en convergence pour les dimensions faibles à modérées.  \n",
    "Cependant, lorsque la dimension $d$ augmente, les séquences de faible discrépance comme Sobol ou Halton deviennent moins efficaces, car elles sont optimisées pour des espaces de faible dimension.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
