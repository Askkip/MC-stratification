{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import qmc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9510565162951535)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(u):\n",
    "    \"\"\"\n",
    "    u est un vecteur de taille d>=1 -> np.array\n",
    "    \"\"\"\n",
    "    d = len(u)\n",
    "    somme = np.sum(u)\n",
    "    return np.cos(2*np.pi*(1/d * somme -0.5))\n",
    "\n",
    "f(np.array([1,0.5,0.3,0.4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\int_{[0,1]^d}^{}{f(u)du}  \\simeq \\frac{1}{N}\\sum_{i=1}^{N}f(u_{(i)}) \\text{ with } u_{(i)} \\sim^{iid} U([0,1]^d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Célian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\stats\\_qmc.py:958: UserWarning: The balance properties of Sobol' points require n to be a power of 2.\n",
      "  sample = self._random(n, workers=workers)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension d = 1\n",
      "  N = 100: Monte Carlo = 0.020523, Quasi-Monte Carlo = 0.000510\n",
      "  N = 1000: Monte Carlo = 0.039154, Quasi-Monte Carlo = 0.000804\n",
      "  N = 10000: Monte Carlo = 0.000285, Quasi-Monte Carlo = 0.000013\n",
      "  N = 20000: Monte Carlo = -0.001585, Quasi-Monte Carlo = -0.000002\n",
      "\n",
      "Dimension d = 2\n",
      "  N = 100: Monte Carlo = 0.310479, Quasi-Monte Carlo = 0.416294\n",
      "  N = 1000: Monte Carlo = 0.411838, Quasi-Monte Carlo = 0.404212\n",
      "  N = 10000: Monte Carlo = 0.392037, Quasi-Monte Carlo = 0.405347\n",
      "  N = 20000: Monte Carlo = 0.405744, Quasi-Monte Carlo = 0.405289\n",
      "\n",
      "Dimension d = 3\n",
      "  N = 100: Monte Carlo = 0.573709, Quasi-Monte Carlo = 0.581031\n",
      "  N = 1000: Monte Carlo = 0.587956, Quasi-Monte Carlo = 0.565722\n",
      "  N = 10000: Monte Carlo = 0.562386, Quasi-Monte Carlo = 0.565682\n",
      "  N = 20000: Monte Carlo = 0.564922, Quasi-Monte Carlo = 0.565672\n",
      "\n",
      "Dimension d = 5\n",
      "  N = 100: Monte Carlo = 0.677379, Quasi-Monte Carlo = 0.710703\n",
      "  N = 1000: Monte Carlo = 0.707595, Quasi-Monte Carlo = 0.715972\n",
      "  N = 10000: Monte Carlo = 0.716421, Quasi-Monte Carlo = 0.716563\n",
      "  N = 20000: Monte Carlo = 0.715804, Quasi-Monte Carlo = 0.716438\n",
      "\n",
      "Dimension d = 10\n",
      "  N = 100: Monte Carlo = 0.869670, Quasi-Monte Carlo = 0.850426\n",
      "  N = 1000: Monte Carlo = 0.851148, Quasi-Monte Carlo = 0.846967\n",
      "  N = 10000: Monte Carlo = 0.846096, Quasi-Monte Carlo = 0.847961\n",
      "  N = 20000: Monte Carlo = 0.848137, Quasi-Monte Carlo = 0.847862\n",
      "\n",
      "Dimension d = 100\n",
      "  N = 100: Monte Carlo = 0.982145, Quasi-Monte Carlo = 0.986949\n",
      "  N = 1000: Monte Carlo = 0.983890, Quasi-Monte Carlo = 0.983675\n",
      "  N = 10000: Monte Carlo = 0.983697, Quasi-Monte Carlo = 0.983637\n",
      "  N = 20000: Monte Carlo = 0.983847, Quasi-Monte Carlo = 0.983680\n",
      "\n",
      "Dimension d = 200\n",
      "  N = 100: Monte Carlo = 0.992515, Quasi-Monte Carlo = 0.990283\n",
      "  N = 1000: Monte Carlo = 0.992248, Quasi-Monte Carlo = 0.991885\n",
      "  N = 10000: Monte Carlo = 0.991768, Quasi-Monte Carlo = 0.991809\n",
      "  N = 20000: Monte Carlo = 0.991889, Quasi-Monte Carlo = 0.991818\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Monte Carlo standard\n",
    "def monte_carlo_integration(d, N):\n",
    "    samples = np.random.uniform(0, 1, (N, d))\n",
    "    return np.mean(np.array([f(sample) for sample in samples]))\n",
    "\n",
    "# Quasi-Monte Carlo (Sobol sequence)\n",
    "def quasi_monte_carlo_integration(d, N):\n",
    "    sobol = qmc.Sobol(d, scramble=True)\n",
    "    samples = sobol.random(N)\n",
    "    return np.mean(np.array([f(sample) for sample in samples]))\n",
    "\n",
    "# Différentes valeurs de d et N\n",
    "d_values = [1, 2, 3,5, 10, 100, 200]\n",
    "N_values = [100, 1000, 10000, 20000]\n",
    "\n",
    "# Stocker les résultats\n",
    "results = {}\n",
    "\n",
    "for d in d_values:\n",
    "    results[d] = {}\n",
    "    for N in N_values:\n",
    "        mc_result = monte_carlo_integration(d, N)\n",
    "        qmc_result = quasi_monte_carlo_integration(d, N)\n",
    "        results[d][N] = (mc_result, qmc_result)\n",
    "\n",
    "# Affichage des résultats\n",
    "for d in results:\n",
    "    print(f\"Dimension d = {d}\")\n",
    "    for N in results[d]:\n",
    "        mc, qmc_res = results[d][N]\n",
    "        print(f\"  N = {N}: Monte Carlo = {mc:.6f}, Quasi-Monte Carlo = {qmc_res:.6f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quand $d \\to \\infty$, on remarque que $I \\to 1$ \\\n",
    "Pour la dimension $d=2$ par exemple, on peut calculer l'intégrale double explicitement et on trouve $I = \\frac{4}{\\pi^2} \\simeq 0.40528$ et ça nous permet de remarquer que QMC converge plus vite vers le bon résultat que MC.\\\n",
    "De même pour $d=1$, $I = 0$ par changement de variable, et on voit la vitesse de convergence plus rapide de QMC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "e_{0,k} = \\left\\{ \\left( \\frac{2j_1 + 1}{2k}, \\dots, \\frac{2j_s + 1}{2k} \\right) \n",
    "\\text{ s.t. } (j_1, \\dots, j_s) \\in \\{0, \\dots, k-1\\}^s \\right\\}\n",
    "\\ $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$\\\n",
    "\\hat{I}_{1,k}(f) := \\frac{1}{k^s} \\sum_{c \\in \\mathcal{e}_0,k} f(c + U_c), \\quad U_c \\sim \\mathcal{U} \\left( \\left[ -\\frac{1}{2k}, \\frac{1}{2k} \\right]^s \\right)\n",
    "\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def haber_estimator_1(f, k, s):\n",
    "    \"\"\"\n",
    "    Estimateur de Haber (ordre 1) avec gestion mémoire efficace.\n",
    "\n",
    "    Paramètres :\n",
    "        f : fonction à intégrer\n",
    "        k : nombre de subdivisions par dimension\n",
    "        s : dimension de l'espace\n",
    "\n",
    "    Retour :\n",
    "        Estimation de l'intégrale de f sur [0,1]^s\n",
    "    \"\"\"\n",
    "    total = 0.0\n",
    "    count = 0\n",
    "    shape = (k,) * s          # ex: (4, 4, 4)      \n",
    "\n",
    "    # Pour chaque cellule du pavage (k^s cases)\n",
    "    for idx in np.ndindex(*shape):\n",
    "        #print(f\"idx : {idx}\")\n",
    "        center = (np.array(idx) + 0.5) / k  # Centre de la cellule\n",
    "        #print(f\"centers : {center}\")\n",
    "        u = np.random.uniform(-0.5/k, 0.5/k, size=s)  # Perturbation uniforme\n",
    "        total += f(center + u)\n",
    "        count += 1\n",
    "\n",
    "    return total / count\n",
    "\n",
    "\n",
    "\n",
    "def haber_estimator_2(f, k, s):\n",
    "    \"\"\"\n",
    "    Estimateur de Haber (ordre 2) avec boucle explicite pour réduire la mémoire.\n",
    "\n",
    "    Paramètres :\n",
    "        f : fonction à intégrer\n",
    "        k : nombre de subdivisions par dimension\n",
    "        s : dimension de l'espace\n",
    "\n",
    "    Retour :\n",
    "        Estimation de l'intégrale de f sur [0,1]^s\n",
    "    \"\"\"\n",
    "    total = 0.0\n",
    "    count = 0\n",
    "    shape = (k,)*s\n",
    "    for idx in np.ndindex(*shape):\n",
    "        center = (np.array(idx) + 0.5) / k\n",
    "        u = np.random.uniform(-0.5/k, 0.5/k, size=s)\n",
    "        total += (f(center + u) + f(center - u)) / 2\n",
    "        count += 1\n",
    "\n",
    "    return total / count\n",
    "\n",
    "\n",
    "\n",
    "# # Test function\n",
    "# def test_function(x):\n",
    "#     return np.cos(2 * np.pi * (np.mean(x, axis=1) - 0.5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension d = 1\n",
      "k = 10, d= 1,number of centers = 10: Haber1 = 0.028707, Haber2 = 0.000582\n",
      "k = 20, d= 1,number of centers = 20: Haber1 = 0.022451, Haber2 = -0.001098\n",
      "k = 50, d= 1,number of centers = 50: Haber1 = 0.001230, Haber2 = 0.000018\n",
      "k = 100, d= 1,number of centers = 100: Haber1 = -0.000868, Haber2 = -0.000004\n",
      "\n",
      "Dimension d = 2\n",
      "k = 10, d= 2,number of centers = 100: Haber1 = 0.412161, Haber2 = 0.403614\n",
      "k = 20, d= 2,number of centers = 400: Haber1 = 0.404865, Haber2 = 0.405311\n",
      "k = 50, d= 2,number of centers = 2500: Haber1 = 0.405131, Haber2 = 0.405283\n",
      "k = 100, d= 2,number of centers = 10000: Haber1 = 0.405277, Haber2 = 0.405284\n",
      "\n",
      "Dimension d = 3\n",
      "k = 10, d= 3,number of centers = 1000: Haber1 = 0.566761, Haber2 = 0.565630\n",
      "k = 20, d= 3,number of centers = 8000: Haber1 = 0.565737, Haber2 = 0.565593\n",
      "k = 50, d= 3,number of centers = 125000: Haber1 = 0.565608, Haber2 = 0.565595\n",
      "k = 100, d= 3,number of centers = 1000000: Haber1 = 0.565594, Haber2 = 0.565596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_values = [1, 2, 3]\n",
    "k_values = [10, 20, 50, 100]  # Number of subintervals per dimension\n",
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "for d in d_values:\n",
    "    results[d] = {}\n",
    "    for k in k_values:\n",
    "        I1 = haber_estimator_1(f, k, d)\n",
    "        I2 = haber_estimator_2(f, k, d)\n",
    "        results[d][k] = (I1,I2)\n",
    "\n",
    "# Affichage des résultats\n",
    "for d in results:\n",
    "    print(f\"Dimension d = {d}\")\n",
    "    for k in results[d]:\n",
    "        mc, qmc_res = results[d][k]\n",
    "        print(f\"k = {k}, d= {d},number of centers = {k**d}: Haber1 = {mc:.6f}, Haber2 = {qmc_res:.6f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence d'$ Haber1$ et d'$Haber2$ vers la vraie valeur de $I$ beaucoup plus rapide que QMC et MC, par exemple en dimension 1 dès $N=20$ on est est déjà proche de 0 alors qu'il fallait attendre $N=100$ pour MC et QMC. \n",
    "\n",
    "On observe aussi une convergence plus rapide de la variance d'$Haber2$ grâce à la technique de réduction de variance (Antithetic Variable).\n",
    "\n",
    "Plus k augmente, plus N augmente et plus l'estimateur d'Haber converge vers la valeur de l'intégrale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension d = 5\n",
      "  k = 2, d= 5,number of centers = 32: Haber1 = 0.781541, Haber2 = 0.717598\n",
      "  k = 3, d= 5,number of centers = 243: Haber1 = 0.710370, Haber2 = 0.716102\n",
      "  k = 4, d= 5,number of centers = 1024: Haber1 = 0.718984, Haber2 = 0.717322\n",
      "\n",
      "Dimension d = 10\n",
      "  k = 2, d= 10,number of centers = 1024: Haber1 = 0.842655, Haber2 = 0.847404\n",
      "  k = 3, d= 10,number of centers = 59049: Haber1 = 0.847854, Haber2 = 0.847707\n",
      "  k = 4, d= 10,number of centers = 1048576: Haber1 = 0.847896, Haber2 = 0.847857\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_values = [5,10]\n",
    "k_values = [2,3,4]  # Number of subintervals per dimension\n",
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "for d in d_values:\n",
    "    results[d] = {}\n",
    "    for k in k_values:\n",
    "        I1 = haber_estimator_1(f, k, d)\n",
    "        I2 = haber_estimator_2(f, k, d)\n",
    "        results[d][k] = (I1,I2)\n",
    "\n",
    "# Affichage des résultats\n",
    "for d in results:\n",
    "    print(f\"Dimension d = {d}\")\n",
    "    for k in results[d]:\n",
    "        mc, qmc_res = results[d][k]\n",
    "        print(f\"  k = {k}, d= {d},number of centers = {k**d}: Haber1 = {mc:.6f}, Haber2 = {qmc_res:.6f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension d = 10\n",
      "  k = 2, d= 10,number of centers = 1024: Haber1 = 0.841889, Haber2 = 0.848720\n",
      "  k = 3, d= 10,number of centers = 59049: Haber1 = 0.848227, Haber2 = 0.847982\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_values = [10]\n",
    "k_values = [2,3]  # Number of subintervals per dimension\n",
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "for d in d_values:\n",
    "    results[d] = {}\n",
    "    for k in k_values:\n",
    "        I1 = haber_estimator_1(f, k, d)\n",
    "        I2 = haber_estimator_2(f, k, d)\n",
    "        results[d][k] = (I1,I2)\n",
    "\n",
    "# Affichage des résultats\n",
    "for d in results:\n",
    "    print(f\"Dimension d = {d}\")\n",
    "    for k in results[d]:\n",
    "        mc, qmc_res = results[d][k]\n",
    "        print(f\"  k = {k}, d= {d},number of centers = {k**d}: Haber1 = {mc:.6f}, Haber2 = {qmc_res:.6f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cependant, les estimateurs d'Haber ne sont pas adaptés quand la dimension explose ($d>>10$), parce que le nombre de centre $n=k^{d}$ est très important. Une voie d'exploration serait l'utilisation de rectangle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Understanding the Approximate Distribution \n",
    "\n",
    "We analyze the distribution of the quantity:\n",
    "\n",
    "$$\n",
    "S_d = \\frac{1}{d} \\sum_{i=1}^{d} u_i\n",
    "$$\n",
    "\n",
    "when $( d )$ is large. Given that $( u_i \\sim U(0,1) )$ (i.i.d.), we apply the **Central Limit Theorem (CLT)**. Since the expectation and variance of a uniform $( U(0,1) )$ variable are:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[u_i] = \\frac{1}{2}, \\quad \\text{Var}[u_i] = \\frac{1}{12},\n",
    "$$\n",
    "\n",
    "we obtain:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[S_d] = \\frac{1}{d} \\sum \\mathbb{E}[u_i] = \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Var}[S_d] = \\frac{1}{d^2} \\sum \\text{Var}[u_i] = \\frac{1}{12d}\n",
    "$$\n",
    "Then we have,\n",
    "\n",
    "$$ \\sqrt{d} \\times \\frac{S_{d} - \\frac{1}{2}}{1/12} \\simeq \\mathcal{N} \\left( 0, 1 \\right)$$\n",
    "\n",
    "Thus, for large $( d )$, we approximate:\n",
    "\n",
    "$$\n",
    "S_d \\sim \\mathcal{N} \\left( \\frac{1}{2}, \\frac{1}{12d} \\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step  2 : Importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance Sampling\n",
    "$$\\mathbb{E}_{x \\sim P}[f(x)] = \\mathbb{E}_{x \\sim Q}\\Big[f(x)\\frac{P(x)}{Q(x)}\\Big]$$\n",
    "Which means  $\\mathbb{E}_{x \\sim P}[f(x)] \\approx \\frac{1}{n}\\sum_{i=1}^nf(x_i)\\frac{P(x_i)}{Q(x_i)}$ where $x_i$ are drawn from $Q$. This applies when $P$ and $Q$ are both normalized. For unnormalized case (not our case)   \n",
    "$$\\mathbb{E}_{x \\sim P}[f(x)] \\approx \\frac{\\sum_{i=1}^nf(x_i)\\frac{P(x_i)}{Q(x_i)}}{\\sum_{i=1}^n\\frac{P(x_i)}{Q(x_i)}}$$  \n",
    "Let the proposal distribution $Q(x)$ be a normal distribution.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal, norm\n",
    "\n",
    "def uniform_pdf(u,d):\n",
    "    \"\"\"\n",
    "    Densité de probabilité pour une distribution uniforme sur [0,1]^d.\n",
    "    Comme c'est uniforme, la densité est constante et vaut 1 sur l'espace.\n",
    "    \"\"\"\n",
    "    if np.all((u >= 0) & (u <= 1)):  # Vérifie que u est bien dans [0,1]^d\n",
    "        return 1\n",
    "    else:\n",
    "        return 0  # En dehors de [0,1]^d, la densité est nulle\n",
    "\n",
    "def normal_pdf(u,d):\n",
    "    \"\"\"\n",
    "    Densité de probabilité pour une loi normale multidimensionnelle de dimension d  : N(mu, Sigma)\n",
    "    avec mu = (1/2, ..., 1/2) et Sigma = (1/12d) * I_d (matrice identité).\n",
    "    \"\"\"\n",
    "    mu = np.full(d, 0.5)  # Vecteur moyenne [1/2, 1/2, ..., 1/2]\n",
    "    sigma2 = 1 / (12 * d)  # Variance pour chaque variable\n",
    "    sigma = np.eye(d) * sigma2  # Matrice de covariance diagonale\n",
    "\n",
    "    return multivariate_normal.pdf(u, mean=mu, cov=sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IS_MC(N,d, mu=1/2, sigma = None):\n",
    "    if not sigma :\n",
    "        sigma = 1/(12*d)\n",
    "    somme = 0\n",
    "    for i in range(N):\n",
    "        sample = np.random.multivariate_normal(np.full(d, 0.5), np.eye(d) *1 / (12 * d))\n",
    "        somme += f(sample) * (uniform_pdf(sample,d)/normal_pdf(sample,d))\n",
    "    return somme/N\n",
    "\n",
    "def IS_QMC(N,d, mu=1/2, sigma = None):\n",
    "    if not sigma :\n",
    "        sigma = 1/(12*d)\n",
    "    \"\"\" Importance Sampling avec Quasi-Monte Carlo utilisant une séquence Sobol \"\"\"\n",
    "    sampler = qmc.Sobol(d, scramble=True)  # Génère des points Sobol dans [0,1]^d\n",
    "    U = sampler.random(N)  # Matrice (N, d) avec des points uniformes\n",
    "    # Transformation en loi normale N(0.5, 1/(12d)) avec la fonction quantile\n",
    "    mu = 0.5\n",
    "    sigma = np.sqrt(1 / (12 * d))\n",
    "    X = norm.ppf(U, loc=mu, scale=sigma)  # Transforme U ~ U[0,1] en X ~ N(mu, sigma^2)\n",
    "    \n",
    "\n",
    "\n",
    "    # Importance Sampling\n",
    "    somme = 0\n",
    "    for x in X:\n",
    "        somme += f(x) * (uniform_pdf(x,d) /normal_pdf(x, d))  \n",
    "    return somme / len(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension d = 1\n",
      "  N = 1000: IS MC = 0.033919, IS QMC = 0.002408\n",
      "  N = 5000: IS MC = 0.008306, IS QMC = -0.000351\n",
      "  N = 10000: IS MC = 0.023044, IS QMC = -0.000293\n",
      "  N = 20000: IS MC = 0.004628, IS QMC = -0.000011\n",
      "  N = 50000: IS MC = -0.000294, IS QMC = 0.000003\n",
      "\n",
      "Dimension d = 2\n",
      "  N = 1000: IS MC = 0.393438, IS QMC = 0.390166\n",
      "  N = 5000: IS MC = 0.412456, IS QMC = 0.422232\n",
      "  N = 10000: IS MC = 0.438745, IS QMC = 0.405676\n",
      "  N = 20000: IS MC = 0.404754, IS QMC = 0.401684\n",
      "  N = 50000: IS MC = 0.418245, IS QMC = 0.403757\n",
      "\n",
      "Dimension d = 3\n",
      "  N = 1000: IS MC = 0.494730, IS QMC = 0.603534\n",
      "  N = 5000: IS MC = 0.534864, IS QMC = 0.569642\n",
      "  N = 10000: IS MC = 0.531637, IS QMC = 0.601552\n",
      "  N = 20000: IS MC = 0.500053, IS QMC = 0.565288\n",
      "  N = 50000: IS MC = 0.597192, IS QMC = 0.551656\n",
      "\n",
      "Dimension d = 5\n",
      "  N = 1000: IS MC = 0.301219, IS QMC = 0.562003\n",
      "  N = 5000: IS MC = 0.406448, IS QMC = 0.451629\n",
      "  N = 10000: IS MC = 0.470515, IS QMC = 0.500818\n",
      "  N = 20000: IS MC = 0.483931, IS QMC = 0.418338\n",
      "  N = 50000: IS MC = 0.560759, IS QMC = 0.799621\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_values = [1, 2, 3, 5]\n",
    "N_values = [1000, 5000, 10000, 20000, 50000]  # Number of subintervals per dimension\n",
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "for d in d_values:\n",
    "    results[d] = {}\n",
    "    for N in N_values:\n",
    "        I1 = IS_MC( N, d)\n",
    "        I2 = IS_QMC( N, d)\n",
    "        results[d][N] = (I1,I2)\n",
    "\n",
    "# Affichage des résultats\n",
    "for d in results:\n",
    "    print(f\"Dimension d = {d}\")\n",
    "    for k in results[d]:\n",
    "        mc, qmc_res = results[d][k]\n",
    "        print(f\"  N = {k}: IS MC = {mc:.6f}, IS QMC = {qmc_res:.6f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en d=5 ça devrait tendre vers 0.76 mais la comportement est quelque peu chaotique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension d = 20\n",
      "  N = 10000: IS MC = 0.000000, IS QMC = 0.000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_values = [20]\n",
    "N_values = [10000]  # Number of subintervals per dimension\n",
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "for d in d_values:\n",
    "    results[d] = {}\n",
    "    for N in N_values:\n",
    "        I1 = IS_MC( N, d)\n",
    "        I2 = IS_QMC( N, d)\n",
    "        results[d][N] = (I1,I2)\n",
    "\n",
    "# Affichage des résultats\n",
    "for d in results:\n",
    "    print(f\"Dimension d = {d}\")\n",
    "    for k in results[d]:\n",
    "        mc, qmc_res = results[d][k]\n",
    "        print(f\"  N = {k}: IS MC = {mc:.6f}, IS QMC = {qmc_res:.6f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problème apparent !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problèmes et solutions pour l'Importance Sampling\n",
    "\n",
    "## 1. Mauvais choix de la densité d'importance\n",
    "L'Importance Sampling (IS) est très sensible au choix de la densité d'importance $g(x)$.  \n",
    "Si $g(x)$ ne couvre pas bien les régions où $f(x)$ est significatif, les pondérations définies par :\n",
    "\n",
    "$$ w(x) = \\frac{p(x)}{g(x)} $$\n",
    "\n",
    "peuvent devenir extrêmement variables, entraînant une grande variance de l'estimateur.  \n",
    "En grande dimension, une loi normale centrée en $0.5$ peut mal représenter les régions importantes de $f(x)$, ce qui engendre une instabilité des résultats.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Problème de dégénérescence en grande dimension\n",
    "Lorsque la dimension $d$ augmente, la plupart des échantillons issus de $g(x)$ se retrouvent éloignés des régions où $f(x)$ est significatif.  \n",
    "Cela conduit à une situation où très peu d'échantillons contribuent réellement à l'intégrale, ce qui augmente la variance et rend l'estimateur instable.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Inefficacité des méthodes Quasi-Monte Carlo en grande dimension\n",
    "Les méthodes Quasi-Monte Carlo (QMC) offrent un gain en convergence pour les dimensions faibles à modérées.  \n",
    "Cependant, lorsque la dimension $d$ augmente, les séquences de faible discrépance comme Sobol ou Halton deviennent moins efficaces, car elles sont optimisées pour des espaces de faible dimension.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
